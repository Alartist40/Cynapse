# Credits & Acknowledgments

Cynapse is built upon the shoulders of giants. We gratefully acknowledge the following research papers, projects, and inspirations.

## Research Papers

*   **"Attention Is All You Need"** (Vaswani et al., 2017) - Foundation of the Transformer architecture used in Elara.
*   **"Llama 2: Open Foundation and Fine-Tuned Chat Models"** (Touvron et al., 2023) - Inspiration for our model architecture and quantization strategies.
*   **"TiDAR: Think in Diffusion, Act in Regression"** - Conceptual basis for our dual-head architecture (though implemented in simplified form).
*   **"Building Effective Agents"** (Anthropic, 2024) - Primary reference for the HiveMind 2.0 multi-agent architecture.
*   **"Shamir's Secret Sharing"** (Adi Shamir, 1979) - Mathematical foundation for the "Bat" neuron's USB shard authentication.

## Open Source Projects

*   **[Ollama](https://github.com/ollama/ollama)** - For making local LLM inference accessible and easy.
*   **[n8n](https://github.com/n8n-io/n8n)** - Workflow automation inspiration for HiveMind.
*   **[Rich](https://github.com/Textualize/rich)** - For the beautiful terminal rendering capabilities.
*   **[PyTorch](https://pytorch.org/)** - The deep learning framework powering Elara.
*   **[Hugging Face Transformers](https://github.com/huggingface/transformers)** - State-of-the-art machine learning.

## Design Inspirations

*   **Claude Code** - For the "Lead Agent + Subagent" orchestration pattern and artifact passing mechanism.
*   **Cyberpunk 2077** - Aesthetic inspiration for the Synaptic Fortress TUI.
*   **Mr. Robot** - Thematic inspiration for the "Ghost Shell" security model.

## Special Thanks

*   **The Cynapse Community** - For testing, feedback, and contributions.
*   **Anthropic** - For publishing detailed research on agent architectures that guided our implementation.

---

*"We are all apprentices in a craft where no one ever becomes a master."* â€” Ernest Hemingway
